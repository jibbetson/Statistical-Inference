---
title: "Lecture notes"
author: "James Ibbetson"
date: "October 25, 2017"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
```

## Probability module
A probability measure, $P$, is a function from the collection of possible events so that the following hold:

1. $0 \leq P(E) \leq 1$ 
2. $P(\text{all possible events}) = 1$ (i.e. something must happen)
3. If $E_1$ and $E_2$ are mutually exclusive events $P(E_1 \cup E_2) = P(E_1) + P(E_2)$. This implies $P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)$ where the ${A_i}$ are mutually exclusive.

These are the 3 basic rules of probability. Also known as Kolmogorov's 3 rules.

Some consequences:  
- The probability of at least one of two (or more) things that cannot simultaneously occur (mutually exclusive) is the sum of their respective probabilities.  
- For two events, the probability that at least one occurs is the sum of their probabilities minus their intersection (i.e. both occur). In math speak, $P(A \cup B) = P(A) + P(B) - P(A \cap B)$.  

A probability mass function, $p$, evaluated at a value $x$ is the probability that a random variable takes that value. It applies to discrete values.  

1. $p(x) \geq 0$ for all $x$.
2. $\sum_{x} p(x) = 1$, where the sum is taken over all possible values for $x$.

Example: the Bernouilli distribution, where $p(x) = \theta^{x} (1 - \theta)^{1-x}$. In the case of two possible outcomes, 0 and 1, then $p(1)=\theta$ and $p(0)=1-\theta$.


###Probability density functions
A probability density function, *pdf*, is equivalent to probability mass function except for random variables whose values exist **on a continuum**. In this case, areas under *pdfs* correspond to probabilities. To be a valid function $f$ must satisfy:

1. $f(x) \geq 0$ for all $x$  
2. $\int{f(x)dx} = 1$

The **probability density function** of the so-called beta distribution is $f(x;\alpha, \beta)=\text{constant}\cdot{x^{\alpha-1}}(1-x)^{\beta-1}$. The shape of this curve changes with the constants $\alpha$ and $\beta$ and is defined on $1\geq x \geq 0$. For a simple linear distribution use $\alpha=2$ and $\beta=1$.

The **cumulative distribution function** of a random variable, $X$, is the probability the random variable is less than or equal to the value $x$. This is written as $F(x) = P(X\leq{x})$.  

In `R` the `pbeta(x, a, b)` function gives the cumulative probability for the beta distribution defined by $\alpha=a$ and $\beta=b$. The `R` function for the beta distribution itself is `beta(a, b)`. Note that `x` can be a list, in which case the function returns a list.

The **survival function** of a random variable $X$ is the probability the random variable is greater then the value $x$, defined as $S(x) = P(X > x)$ or $S(x) = 1 - F(x)$.

**Quantiles** are related to probability distribution functions. Specifically, the $\alpha$th quantile of $F(x)$ is the value of $x$ such that $F(x_\alpha)=\alpha$. That is, what value of $x$ should we choose such that the area under $f(x)$ up to $x$ is equal to $\alpha$?  

In `R` use the function `qbeta(x, a, b)` for quantiles of the beta distribution function.

Percentiles are simply quantiles with $\alpha$ expressed as a percent. 


## Conditional probability module
Conditional probability is when additional information is provided, i.e. conditions, which changes the probability of an event occurring. For example, if you roll a die and are told that the value is odd then the probability of having rolled a $1$ is no longer $\frac{1}{6}$ but $\frac{1}{3}$.

Let $B$ be an event so that $P(B) > 0$. The probability of an event $A$ given that $B$ has occurred is written as $P(A ~|~ B)$. Then the conditional probability of an event $A$ given that $B$ has occurred is:  

$P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}$  

If $A$ and $B$ are independent then $P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)$.

###Bayes' rule & diagnostic tests
**Bayes' rule** is a famous example of conditional probability in cases where we know some marginal probabilities. It is formally written as:  

$P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}$.  

The $B^c$ notation is shorthand for not $B$, and by definition (since they are mutually exclusive) $P(B^c) = 1 - P(B)$.

Bayes' rule is very useful in diagnostic tests, where the result of the test can be either positive $+$ or negative $-$.  

The **prevalence of the disease** is the marginal probability of disease, $P(D)$.  

The **sensitivity** is defined as the probability that a test for the disease is positive given that the subject actually **has** the disease, $P(+ ~|~ D)$.  

The **specificity** is defined as the probability that a test is negative given that the subject does **not have** the disease, $P(- ~|~ D^c)$.  

The **positive predictive value** is the probability that the subject has a disease given that the test is positive, $P(D ~|~ +)$.  

The **negative predictive value** is the probability that the subject does not have a disease given that the test is negative, $P(D^c ~|~ -)$.  

The **diagnostic likelihood ratio of a positive test**, labeled $DLR_+$, is $P(+ ~|~ D) / P(+ ~|~ D^c)$, which is the $sensitivity / (1 - specificity)$.  

The **diagnostic likelihood ratio of a negative test**, labeled $DLR_-$, is $P(- ~|~ D) / P(- ~|~ D^c)$, which is the $(1 - sensitivity) / specificity$.  

### IID random variables
Event $A$ is **independent** of $B$ if $P(A ~|~ B) = P(A)$, or equivalently if $P(A \cap B) = P(A)P(B)$.  

Random variables are said to be **iid** if they are **independent and identically distributed**:

- Independent: statistically unrelated from one and another  
- Identically distributed: all having been drawn from the same population distribution.  

Iid random variables are the default model for random samples.


## Expected values module
Expected values are useful for characterizing a distribution so that we don't have to talk about all the details of the distribution function. The mean is a characterization of the center of a distribution, while the variance and standard deviation are characterizations of how spread out it is.  

The **expected value** or **mean** of a random variable is the center of its distribution. For discrete random variable, $X$, with PMF $p(x)$, the mean is defined as $E[X] = \sum_x xp(x)$, where the sum is taken over the possible values of $x$.

In a sense, $E[X]$ represents the *center of mass* of a collection of locations and weights, $\{x, p(x)\}$

Example 1: Suppose an unbiased coin is flipped and $X$ is defined as $0$ or $1$ corresponding to a head or a tail, respectively. What is the expected value of $X$?  

Since $p(x) = 0.5$, $E[X] = 0.5 \times 0 + 0.5 \times 1 = 0.5$  

Example 2: What about a biased coin? Suppose that a random variable, $X$, where $P(X=1) = p$ and $P(X=0) = (1 - p)$ and $p\neq 0.5$.  

Its expected value is $E[X] = 0 * (1 - p) + 1 * p = p$

Example 3: Suppose that a die is rolled and $X$ is the number face up. What is the expected value of $X$?  

$E[X] = 1 \times \frac{1}{6} + 2 \times \frac{1}{6} + 3 \times \frac{1}{6} + 4 \times \frac{1}{6} + 5 \times \frac{1}{6} + 6 \times \frac{1}{6} = 3.5$  

Thinking in terms of center or mass, a geometric argument makes this answer obvious without calculation.

```{r die_rolls, fig.align='left', fig.height=4, fig.width=8}
ggplot(data.frame(x = factor(1 : 6), y = rep(1/6, 6)), aes(x = x, y = y)) + geom_bar(stat = "identity", colour = 'black', fill = "lightblue")
```

For a continuous random variable, $X$, with density $f(x)$, the expected value is again exactly the center of mass of the density.  

### The sample mean
The **sample** expected values (the sample mean and variance) will
estimate the population versions. The larger the sample size, the better the estimation.

The center of mass of the data is the empirical mean of our sample, $\bar X = \sum_{i=1}^n x_i p(x_i)$ where $p(x_i) = 1/n$. Note that the probability of each *sample* event is assumed to be identical, which does **not** imply the probability of each outcome is identical.

### Facts about expected values
The **average** of a random variable is itself a random variable and its associated distribution has an expected value.  

The center of this distribution is the same as that of the original distribution.  

Therefore, the expected value of the **sample mean** is the population mean that it's trying to estimate.

When the expected value of an estimator is what its trying to estimate, we say that the estimator is **unbiased**.

### Simulation experiment
Consider a simulation experiment in which we compare the expected value of x die rolls. We could do a large number of single die rolls and get a sample distribution (roughly equal across $x=1:6$). And we can compare this distribution with the average of 2, 3, or 4 die rolls.

```{r die_rolls2, fig.align='left', fig.height=4, fig.width=8, warning=FALSE, error=FALSE, message=FALSE}  
nosim <- 10000
dat <- data.frame(
  x = c(sample(1 : 6, nosim, replace = TRUE),
        apply(matrix(sample(1 : 6, nosim * 2, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 3, replace = TRUE), 
                     nosim), 1, mean),
        apply(matrix(sample(1 : 6, nosim * 4, replace = TRUE), 
                     nosim), 1, mean)
        ),
  size = factor(rep(1 : 4, rep(nosim, 4))))
g <- ggplot(dat, aes(x = x, fill = size)) 
g<- g + geom_histogram(alpha = .20, binwidth=.25, colour = "black") 
g + facet_grid(. ~ size)
```
As the number of die rolls increases, the distribution of sample outcomes (the average of the rolls) becomes more and more gaussian with a mean of about what we know it should be ($E[x]=3.5$).

###Summarizing what we know
- Expected values are properties of distributions
- The population mean is the center of mass of population
- The sample mean is the center of mass of the observed data
- The sample mean is an estimate of the population mean
- The sample mean is unbiased 
  - The population mean of its distribution is the mean that it's
  trying to estimate
- The more data that goes into the sample mean, the more 
concentrated its density / mass function is around the population mean


##Variability module
**Variance** is a measure of the spread of the distribution of a population of possible experiment outcomes. If $X$ is a random variable with mean $\mu$, the population variance of $X$ is defined as:   
$Var(X) = E[(X - \mu)^2] = E[X^2] - E[X]^2$.  

The rightmost equation is almost always used in practice. Thus the variance is the expected (squared) distance from the population mean. Densities with a higher variance are more spread out than densities with a lower variance. The square root of the variance is called the **standard deviation**, and is usually denoted as $\sigma$. The main benefit of working with standard deviations is that they have the same units as the data, whereas variance has the units squared.  

For a coin toss:  
$E[X] = 0 \times (1-p) + 1 \times p = p$  
$E[X^2] = E[X] = p$  
$Var(X) = E[X^2] - E[X]^2 = p - p^2 = p(1-p)$  

The **sample** variance is $S^2 = 1/(n-1) \times \sum_{i-1}(X_i - \bar{X})^2$ where n is the number of observations. Note the division by $(n-1)$ not $n$ (something to do with unbiased-ness).   

The sample variance is itself a random variable so it has an associated population distribution. That is, if you conduct the sampling experiment many times, the variance will vary about an average value *whose expected value is just the population variance*.  

Next, let's consider the variation in the distribution of means of random samples from a population, with $n$ observations per sample. If $\bar{X}$ is a sample mean, and $\mu$ the mean of the population, then the expected value of $\bar{X}$ is just $\mu$.  

The variance of the sample mean, $Var(\bar{X})$, is called the **standard error of the mean** and is related to the standard deviation of the population, $\sigma$:  
$Var(\bar{X}) = \sigma^2 / n$.  

**Standard normals** have variance of 1, so means of $n$ standard normals have a standard deviation of $1/\sqrt{n}$.  

###Variance examples
On a computer it's easy to simulate sample variances and distribution of sample variances to verify these claims. A couple of examples:

Start with the normal distribution:  
$f(x) = 1/\sqrt{2\pi\sigma} \times e^{-(x - μ)^2/2 σ^2}$  

```{r var_sim1}
# normal distribution with mean = 0, sd = 1.
# The standard deviation of the mean of 10 observations
# should be 1/sqrt(10) or about 0.32
# so we do 1000 simulations to check
nosim <- 1000
n <- 10
sd(apply(matrix(rnorm(nosim * n), nosim), 1, mean))
```

Next try the poisson distribution:  
$p(x)=\lambda^x exp(-\lambda)/x!$
```{r var_sim2}
# poisson distribution with lambda = 4 = mean & variance.
# The standard deviation of the mean of 10 observations
# should be 2/sqrt(10) or about 0.63
# so we do 1000 simulations to check
nosim <- 1000
n <- 10
sd(apply(matrix(rpois(nosim * n, 4), nosim), 1, mean))
```
###Summarizing variance

- The sample variance estimates the population variance
- The distribution of the sample variance is centered on what its estimating (means the sampling is unbiased)
- It gets more concentrated around the population variance the larger the sample sizes
- The variance of the sample mean is the population variance divided by $n$ (i.e. $Var(\bar{X})=\sigma^2/n$)


##Distribution module
Some important, common distributions.

###Bernouilli
The simplest is the Bernoulli distribution, which arises as a result of a binary outcome. Bernoulli random variables take the values of 1 or 0 with probabilities $p$ and $1-p$ respectively.
Bernouilli mass function is  
$P(X = x) = p^x(1-p)^{1-x}$  
$E[X]$ is $p$ and $Var(X)$ is $1-p$  

###Binomial
A binomial distribution is what you get from multiple (i.e. the sum of) **iid** Bernouilli events. Mathematically, let $X_1,\ldots,X_n$ be iid Bernoulli$(p)$; then $X = \sum_{i=1}^n X_i$ is a binomial random variable. The binomial mass function is:  
$\begin{aligned} P(X = x) = \binom{n}{x} p^x(1 - p)^{n-x} \end{aligned}$  
for $x=0,\ldots,n$ and where $\binom{n}{x} = \frac{n!}{x!(n-x)!}$.  
$E[X]$ is $p$ and $Var(X)$ is $1-p$  

In R use the `choose()`, `pbinom()` and `qbinom()` functions for working with binomial distribution problems.  

###Normal
The most important distribution is the normal distribution or the so-called **bell curve**. Its mass function is the gaussian:  
$f(x) = (2\pi\sigma)^{-1/2}e^{-(x - μ)^2/2 σ^2}$  
$E[X] = \mu \text{ and } Var(X) = \sigma^2$  

Usually written in shorthand as $X \sim N(\mu, \sigma^2)$.  

**Standard** normal distributions are special cases where $\mu = 0, \sigma = 1$.  

The areas under the normal distribution for $\pm1 \sigma$, $\pm2 \sigma$ and $\pm3 \sigma$ from the mean are 68%, 95% and 99%, respectively. Memorize!  

$1.28, 1.645, 1.96 \text{ and } 2.33$ standard deviation units from the mean are the $90^{th}, 95^{th}, 97.5^{th}, \text{ and } 99^{th}$ percentiles, respectively. Memorize!  

In R use the `pnorm()` and `qnorm()` functions when working with normal distributions. E.g. `pnorm(x, mean=mu, sd=sigma, lower.tail=FALSE)`.  

###Poisson
The Poisson distribution is another very useful distribution, which is used to model counts, event-time or survival data, contingency tables. Its mass function is:  
$f(x) = \lambda^x exp(-\lambda)/x!$  
Both $E[X] = \lambda$ and $Var(X) = \lambda$.  

In R use the `ppois()` and `qpois()` functions for working with Poisson distribution problems.  

Poisson random variables are also used to model rates.  
$X \sim Poisson(\lambda t)$ where $\lambda = E[X/t]$ is the expected count per unit of time, and $t$ is the total monitoring time.  

An example: The number of people that show up at a bus stop is Poisson with a mean of $2.5$ per hour. If watching the bus stop for 4 hours, what is the probability that $3$ or fewer people show up for the whole time?  
```{r poisson_rate_ex}
ppois(3, lambda = 2.5 * 4)
```

Poisson distributions are also useful for approximating binomials when $n$ is very large and $p$ is very small. Define $\lambda = np$. 

An example: We flip a coin with success probablity $p=0.01$ five hundred times. What's the probability of 2 or fewer successes?
```{r binom_approx_ex}
pbinom(2, size = 500, prob = 0.01)
```
```{r poisson_approx_ex}
ppois(2, lambda = 500 * 0.01)
```


##Asymptotics module







